### Reflective Activity 1 – Ethics in computing in the age of Generative AI

Generative AI has rapidly transformed industries since late 2022, with Computer Science at the heart of this revolution (Hackningas, 2024). While AI itself is not new, the fast pace of advancements like ChatGPT has raised urgent concerns about governance. Correa et al. (2023) highlight the challenge of establishing a global consensus on AI regulation, as different countries approach the technology with varying priorities. For example, China focuses on state control, while the European Union prioritizes privacy and human rights through regulations like the AI Act.

The unique nature of generative AI, which can autonomously produce content, creates specific challenges around privacy, intellectual property, and misinformation. Correa et al. argue that policymakers need better tools to compare AI governance strategies worldwide. Without common guidelines, countries may create regulations that either limit innovation or fail to prevent harmful AI misuse.

Deckard (2023) suggests a governance model with multiple levels that combines global rules with local flexibility. This approach would let international organizations, like the UN, create general ethical guidelines, while individual countries adjust them to suit their own laws and cultural practices. Regulatory sandboxes could be useful by giving companies a space to safely experiment with AI technologies under supervised conditions (Krishnan, 2023).

The governance of generative AI impacts legal, social, and professional fields. Legally, the question of "ownership" over AI-generated content remains unresolved, especially in cases where it resembles human-made creations  (VP, 2023). Socially, the rise of deepfakes and manipulated media threatens public trust (Kumar, 2023). Professionally, generative AI could disrupt industries by automating tasks, requiring workers to develop new skills such as AI literacy.

To address these challenges, global collaboration is essential. Establishing an international AI framework could provide common ethical standards while allowing for national adaptation. Public awareness campaigns should teach people about the risks and benefits of AI, helping create a better-informed public (Williams, 2024). Additionally, comprehensive legal frameworks must be developed to address intellectual property and data ownership concerns.

By adopting a flexible but structured approach, we can ensure that generative AI evolves in a way that benefits society, minimises risks, and respects legal and ethical boundaries.

### Collab Discussion 1:

The malware disruption case study demonstrates how the ACM Code of Ethics applies when dealing with harmful software. The code tells computing professionals to put the public's well-being first, avoid causing harm, and maintain trust in the field. In this situation, professionals need to act responsibly to stop the malware, ensuring their actions are both ethical and legal.

According to the ACM Code, professionals must work to prevent further harm (ACM, 2018). For example, they should eliminate the malware without making things worse, like violating people's privacy or accessing systems without permission. The code also emphasizes the importance of professionals taking responsibility for their actions (ACM, 2018). This includes working with the right authorities and following legal rules when dealing with malware, especially since it often spreads across different countries. They need to understand and respect both local and international laws.

When comparing this with the British Computer Society (BCS) Code of Conduct, the BCS also highlights the need to protect the public interest and act with integrity (BCS, 2011). Both codes agree that professionals must cooperate with law enforcement and follow legal guidelines. In both cases, the role of professionalism is key, meaning that professionals must be honest, careful, and transparent in their actions to solve the problem without making things worse.

In conclusion, both the ACM and BCS codes guide professionals to handle incidents like malware responsibly. They must protect public trust, act lawfully, and make sure they do not cause additional harm while resolving the issue.

### Reflective Actvity 2

The Cambridge Analytica scandal in 2018 is a notable example of the inappropriate use of surveys to harvest personal data from millions of Facebook users without informed consent. Cambridge Analytica accessed data through seemingly harmless personality quizzes on Facebook. These quizzes collected not only respondents’ data but also that of their entire social network. The information was used for targeted political campaigns during events such as the 2016 U.S. Presidential Election and the Brexit referendum (Confessore, 2018). The ethical violation here was the manipulation of data collection under false pretenses, misleading users about the scope and purpose of data use.

Ethically, this case clearly violates user rights and informed consent. Users did not know their data was being used for political purposes, breaking the rules of honesty and transparency found in the ACM Code of Ethics (ACM, 2018) and the British Computer Society (BCS) Code of Conduct (BCS, 2019). Both stress the importance of protecting privacy and getting clear permission before collecting and using data.

Legally, the scandal revealed gaps in data protection laws, leading to Facebook facing a $5 billion fine from the U.S. Federal Trade Commission (FTC) (Confessore, 2018) and triggering the enforcement of the European Union’s General Data Protection Regulation (GDPR) in 2018. GDPR now mandates stricter rules on data consent and handling, aiming to protect user privacy more effectively (European Union, 2018).

Socially, the scandal damaged public trust in social media platforms, raising worries about how big data can affect democratic processes. The case showed how users were profiled and targeted with personalized political ads without their knowledge, leading to global discussions about the ethical use of data analytics (Confessore, 2018).

Another example of inappropriate survey use is in health apps, where sensitive medical data is collected through "wellness" surveys and sold to third parties. Similarly, biased political polling can shape public opinion by asking questions in a way that leads to certain responses. These cases highlight the ethical need for transparency, consent, and responsible data usage. Legal frameworks like GDPR help ensure accountability, while professional organizations require following ethical standards to protect privacy. Stronger oversight and governance are crucial to ensure the ethical use of survey data, especially on digital platforms (European Union, 2018).

References

ACM (2018) ACM code of ethics and professional conduct. Available at: https://ethics.acm.org/code-of-ethics/ (Accessed: 19 October 2024).

Association for Computing Machinery (ACM). 2018. ACM Code of Ethics and Professional Conduct. Available at: https://www.acm.org/code-of-ethics [Accessed 19 Oct. 2024].

British Computer Society (BCS) (2011) Code of conduct. Available at: https://www.bcs.org/more/code-of-conduct/ (Accessed: 19 October 2024).

British Computer Society (BCS). 2019. BCS Code of Conduct. Available at: https://www.bcs.org/membership/become-a-member/bcs-code-of-conduct/ [Accessed 19 Oct. 2024].

Confessore, N., 2018. Cambridge Analytica and Facebook: The Scandal and the Fallout So Far. The New York Times. Available at: https://www.nytimes.com/2018/04/04/us/politics/cambridge-analytica-scandal-fallout.html [Accessed 19 Oct. 2024].

Correa, J., Montoya, J., Lopez, A. and Perez, D. (2023) 'Global diversity in AI governance: Challenges and opportunities', AI and Ethics, 7(3), pp. 456-470. Available at: https://www.sciencedirect.com/science/article/pii/S2666389923002416 (Accessed: 19 October 2024).

Deckard, R. (2023) What are ethics in AI, BCS. Available at: https://www.bcs.org/articles-opinion-and-research/what-are-ethics-in-ai/ (Accessed: 19 October 2024).

European Union. 2018. General Data Protection Regulation (GDPR). Available at: https://gdpr.eu/ [Accessed 19 Oct. 2024].

Hackningas (2024) The rise of generative AI: Transforming Industries and daily life, Medium. Available at: https://medium.com/@hackningas/the-rise-of-generative-ai-transforming-industries-and-daily-life-f13b4e894390 (Accessed: 21 October 2024).

Krishnan, T.S. (2023) Artificial intelligence sandboxes for regulatory compliance, Medium. Available at: https://medium.com/unframed-thinking/artificial-intelligence-sandboxes-for-regulatory-compliance-17da596622ba (Accessed: 21 October 2024).

Kumar, M. (2023) The rise of Deep Fakes: Are we losing trust in the digital age?, Medium. Available at: https://medium.com/@careerInAI/the-rise-of-deep-fakes-are-we-losing-trust-in-the-digital-age-70906ae44c5c (Accessed: 21 October 2024).

VP, H.M. (2023) The ethics of Generative AI: Copyright, ownership, and authenticity, Medium. Available at: https://medium.com/@hasnamariyamvp/the-ethics-of-generative-ai-copyright-ownership-and-authenticity-bd8364730399 (Accessed: 21 October 2024).

Williams, J. (2024) The growing gap: Public awareness of AI and its potential, Medium. Available at: https://medium.com/@jameszombay/the-growing-gap-public-awareness-of-ai-and-its-potential-039af8087af8 (Accessed: 21 October 2024).
